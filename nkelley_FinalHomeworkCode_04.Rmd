---
title: "nkelley_FinalHomeworkCode_04"
author: "Natalia Kelley"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    toc_float: TRUE
---
# Preliminaries

Load the following packages:

```{r load packages}
library(curl)
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(manipulate)
library(lmodel2)
```

# Part One: Writing a Function

Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data

```{r Z.prop.test function}

Z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, alternative = "two.sided", conf.level = 0.95) 
#specifies arguments
{
    pstar <- ((p2*n2)+(p1*n1))/(n1 + n2)
#pstar is the pooled proportion for both samples
  
    if((n1 * p1 > 5) == FALSE || (n1 * (1-p1) > 5) == FALSE || (n2 * p2 > 5) == FALSE || (n2 * (1-p2) > 5) == FALSE) {
    warning("normal distribution invalid")} 
#here specify that if (\(n * p > 5\) and \(n * (1-p) > 5\)) untrue, run function but print error message that assumptions aren't met.
  
  if(alternative == "two.tailed"){
  Z <- (p2-p1-p0)/(sqrt(pstar(1-pstar)(1/n1 + 1/n2)))
  print(Z)
#Z test statistic for two-sample Z test
  P <- 1 - pnorm(Z, lower.tail = TRUE) + pnorm(Z, lower.tail = FALSE)
  print(P)
#p value for two-sample Z test
lower <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) 
upper <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
     CI <- c(lower, upper)
#confidence interval based on normal distribution  
}

  if(alternative == "less"){
        Z <- (p2-p1)/(sqrt(pstar(1-pstar)(1/n1 + 1/n2)))
#z for p1<p2
    P <- pnorm(Z, lower.tail = TRUE)
#p for p1<p2
    lower <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) # ci intervals
upper <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
     CI <- c(lower, upper)
#CI for p1<p2
} 
#two sample, p1<p2

  if(alternative == "greater"){
    Z <- (p1-p2)/(sqrt(pstar(1-pstar)(1/n1 + 1/n2)))
# z for p1>p2
    P <- pnorm(Z, lower.tail = FALSE)
#p for pw>p2
      lower <- ((p1-p2)-qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1) 
upper <- ((p1-p2)+qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1)
     CI <- c(lower, upper)
#confidence interval for p1>p2 test
} 
#this calcuates one-sided p value of p1>p2

  if(is.null(p2) || is.null(n2)){
    Z <- (p1-p0)/(sqrt(p0(1-p0)/n1))
#Z statistic for one-sample Z test    
    P <- pnorm(Z, two.sided = TRUE)
#p value associated with Z for one-sample Z test
    lower <- p1 - qnorm(conf.level) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm(conf.level) * sqrt(p1 * (1 - p1)/n1)
CI <- c(lower, upper)
#CI for one-sample Z test
}
#here if p2 or n2 or both is NULL, perform one-sample Z test. "||" means [or](http://applied-r.com/conditionals-in-r/)
    
values <- c(Z, P, CI)
print(values)
 
}
 # end of function
```
From Abby: cat( "Z_score =", Z.two.sample <- Z.two.sample, "\n", 
     "CI =", CI.two.sample <- CI.two.sample, "\n", 
     "P-Value =", P.Value <- P.Value, "\n")
}

<<<<<<< HEAD
# Part Two: Regression
=======
*The "alternative" part of the function is just indicating to the function which side of the distribution you are looking at (in a one or two sample test). Everything else in the function should be constant except for the "lower.tail" part of defining p (so basically you dont have to repeat the CI equation each time). Also, don't forget to plug some values into the function to see if it works!*

```{r load packages}
install.packages("tidyverse")

install.packages("ggplot2")

library(curl)

library(ggplot2)

library(tidyverse)
```

```{r load regression packages}

install.packages("gridExtra")
install.packages("manipulate")
install.packages("lmodel2")
```

```{r library}
library(gridExtra)
library(manipulate)
library(lmodel2)
```
>>>>>>> 5ccb87e66541538690635b6f5b6b35367b137a5c

The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r load in Kamilar Cooper data}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")

c <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
#creates data frame from raw csv file

d <- na.omit(c)
```

## Fit Regression and Produce Scatterplot

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r regression and equation}
#normal variables
y <- d$MaxLongevity_m
x <- d$Brain_Size_Species_Mean

m <- lm(y~x, data = d)
summary(m)

t <- coef(summary(m))
t <- data.frame(unlist(t))
colnames(t) <- c("Est", "SE", "t", "p")
t

#this equation is used to add linear regression equation and r2 to ggplot later
eq <- function(x,y) {
  m <- lm(y ~ x)
  as.character(
    as.expression(
      substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,
                list(a = format(coef(m)[1], digits = 4),
                b = format(coef(m)[2], digits = 4),
                r2 = format(summary(m)$r.squared, digits = 3)))
    )
  )
}

#log variables
ly <- log(d$MaxLongevity_m)
lx <- log(d$Brain_Size_Species_Mean)

ml <- lm(ly~lx, data = d)
ml
summary(ml)

lt <- coef(summary(ml))
lt <- data.frame(unlist(lt))
colnames(lt) <- c("Est", "SE", "t", "p")
lt

leq <- function(lx,ly) {
  ml <- lm(ly ~ lx)
  as.character(
    as.expression(
      substitute(italic(ly) == a + b %.% italic(lx)*","~~italic(r)^2~"="~r2,
                list(a = format(coef(ml)[1], digits = 4),
                b = format(coef(ml)[2], digits = 4),
                r2 = format(summary(ml)$r.squared, digits = 3)))
    )
  )
}
```
eq() from https://www.roelpeters.be/how-to-add-a-regression-equation-and-r-squared-in-ggplot2/

```{r normal ggplot linear regression}
#for normal
g <- ggplot(data = d, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm", formula = y ~ x) + 
  geom_text(x = 300, y = 800, label = eq(x, y), parse = TRUE) +
  labs(x = "Brain Size Species Mean", y = "Max Longevity", title = "Brain Size vs Longevity") +
  theme_classic()
g
```

```{r log linear regression ggplot}
#for log
lg <- ggplot(data = d, aes(x = lx, y = ly)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x)  +
  geom_text(x = 4, y = 6.4, label = leq(lx, ly), parse = TRUE) +
  labs(x = "Log Brain Size Species Mean", y = "Log Max Longevity", title = "Log Brain Size vs Log Max Longevity") + theme_classic()
lg

```

<<<<<<< HEAD


**Notes from Class:**
add equation:
victoria's package:  stat_poly_eq(formula = y ~ x, aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")), parse = TRUE) 
requires library(ggpmisc)

for legend: ggplot2 theme command

for colors: scale_color_manual(name = "Intervals - 90%", values=c("Confidence" = "#FF6600", "Prediction" = "#000066"))

## Slope Estimate with Hypothesis Test and CI

Identify and interpret the point estimate of the slope (\(\beta_1\)), as well as the outcome of the test associated with the hypotheses H0: \(\beta_1\) = 0; HA: \(\beta_1\) ≠ 0. Also, find a 90 percent CI for the slope (\(\beta_1\)) parameter.
=======
*check that the intercept from the summary of you model matches that of your graph. it looks like your model in ggplot doesnt match the original (x~y vs y~x)* 

2) Identify and interpret the point estimate of the slope (\(\beta_1\)), as well as the outcome of the test associated with the hypotheses H0: \(\beta_1\) = 0; HA: \(\beta_1\) ≠ 0. Also, find a 90 percent CI for the slope (\(\beta_1\)) parameter.
>>>>>>> 5ccb87e66541538690635b6f5b6b35367b137a5c

```{r id and interpret slope and test, also CI}
t1 <- unlist(m$coefficients)
# normal
beta0<- t$Est[1]
beta0 

beta1 <- t$Est[2]
beta1

ci.slope <- confint(m, level=0.90)
ci.slope

# log

lbeta0<- lt$Est[1]
lbeta0 

lbeta1 <- lt$Est[2]
lbeta1

lci.slope <- confint(ml, level=0.90)
lci.slope

```

## 90% CI Lines and PI Bands

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

```{r normal CI PI plot}
#normal
ci <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = x), interval = "confidence", level = 0.90)

pi <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = x), interval = "prediction", level = 0.90)

df <- data.frame(cbind(x, y, ci, pi))
names(df) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(df)

g1 <- ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(data = df, aes(x = x, y = CIlower, col = "Confidence")) +
  geom_line(data = df, aes(x = x, y = CIupper, col = "Confidence")) +
  geom_line(data = df, aes(x = x, y = PIlower, col = "Prediction")) +
  geom_line(data = df, aes(x = x, y = PIupper, col = "Prediction")) +
  labs(x = "Brain Size Species Mean",
         y = "Max Longevity")
g1
```

```{r log CI PI plot}
#log
lci <- predict(ml, newdata = data.frame(x = lx), interval = "confidence", level = 0.90)

lpi <- predict(ml, newdata = data.frame(x = lx), interval = "prediction", level = 0.90)

ldf <- data.frame(cbind(lx, ly, lci, lpi))
names(ldf) <- c("lx", "ly", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(ldf)

lg1 <- ggplot(data = ldf, aes(x = lx, y = ly)) +
  geom_point()+
  geom_line(data = ldf, aes(x = lx, y = CIlower, colour = "Confidence")) +
  geom_line(data = ldf, aes(x = lx, y = CIupper, colour = "Confidence")) +
  geom_line(data = ldf, aes(x = lx, y = PIlower, colour = "Prediction")) +
  geom_line(data = ldf, aes(x = lx, y = PIupper, colour = "Prediction")) +
  labs(x = "Brain Size Species Mean",
         y = "Max Longevity")
lg1
```

<<<<<<< HEAD
## Point Estimate with 90% PI 

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

=======
*I'm not totally sure what's happening here. You dont need a line for CIfit, but idk why the others arent straight. They are likely above your best fit line due to the inconsistency in model formation that I mentioned above*

4) Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
>>>>>>> 5ccb87e66541538690635b6f5b6b35367b137a5c
```{r 90% PI}
#normal
predict(m, newdata = data.frame(x = 800))
#point estimate

predict(m, newdata = data.frame(x = 800), interval = "prediction", level = 0.90)
#PI

#log
predict(ml, newdata = data.frame(lx = log(800)))
#point estimate

predict(ml, newdata = data.frame(lx = log(800)), interval = "prediction", level = 0.90)
#PI

```
This is an extrapolation, and the r^2 associated with this extrapolation is very low, so I would not trust this prediction. The r^2 for my log predictions is lower though, which is not the expected result. I was expecting the log to be more reliable. I think something may have gone wrong, but I can't figure out what. 

## Model Comparison

**Looking at your two models, which do you think is better? Why?**

I expected the first model to be worse than the log model, but the r^2 for my log model is lower, which is unexpected. I was expecting the log to be more reliable. I think something may have gone wrong, but I can't figure out what. 

# Challenges

1) My prediction lines look very messed up. I think they should be smooth, but I can't figure out what went wrong.
  *Update: when I submitted this, the lines were still messed up. I eventually went back and sifted through until I found the issue (x~y vs y~x discrepancies in my code). 

2) I'm having trouble getting a legend to appear for the different prediction line colors. 
  *Update: while doing my data replication, I learned I could assign categories (ex. prediction vs confidence) rather than colors, and this would automatically assign colors and add an appropriate legend. I added this in here for clarity. 

3) My log model is not giving the expected results, so I'm concerned something went wrong. 
